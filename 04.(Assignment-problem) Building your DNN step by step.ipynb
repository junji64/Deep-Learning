{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‹¬ì¸µ ì‹ ê²½ë§ êµ¬ì¶• : ë‹¨ê³„ë³„\n",
    "\n",
    "4 ì£¼ì°¨ ê³¼ì œ (part 1/2)ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ì´ì „ì— 2ì¸µ ì‹ ê²½ë§(ë‹¨ì¼ ì€ë‹‰ì¸µ í¬í•¨)ì„ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤. ì´ë²ˆ ì£¼ì—ëŠ” ì›í•˜ëŠ” ë§Œí¼ì˜ ë ˆì´ì–´ë¡œ ì‹¬ì¸µ ì‹ ê²½ë§ì„ êµ¬ì¶• í•  ê²ƒì…ë‹ˆë‹¤!\n",
    "\n",
    "* ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ì‹¬ì¸µ ì‹ ê²½ë§ì„ êµ¬ì¶•í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  í•¨ìˆ˜ë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "* ë‹¤ìŒ ê³¼ì œì—ì„œëŠ” ì´ëŸ¬í•œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì‹¬ì¸µ ì‹ ê²½ë§ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì´ ê³¼ì œ í›„ ë‹¤ìŒì„ ìˆ˜í–‰ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤**:\n",
    "\n",
    "* ReLUì™€ ê°™ì€ ë¹„ì„ í˜• ë‹¨ìœ„ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ê°œì„ \n",
    "* ë” ê¹Šì€ ì‹ ê²½ë§ êµ¬ì¶• (1 ê°œ ì´ìƒì˜ ìˆ¨ê²¨ì§„ ë ˆì´ì–´ í¬í•¨)\n",
    "* ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ì‹ ê²½ë§ í´ë˜ìŠ¤ êµ¬í˜„\n",
    "\n",
    "\n",
    "**í‘œê¸°ë²•**:\n",
    "\n",
    "* ìœ„ì²¨ì $ [l] $ëŠ” $ l^{th} $ ë ˆì´ì–´ì™€ ê´€ë ¨ëœ ìˆ˜ëŸ‰ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "  - ì˜ˆ : $ a^{[L]} $ëŠ” $ L^{th} $ ë ˆì´ì–´ í™œì„±í™”ì…ë‹ˆë‹¤. $ W^{[L]} $ ë° $ b^{[L]} $ëŠ” $ L^{th} $ ë ˆì´ì–´ ë§¤ê°œ ë³€ìˆ˜ì…ë‹ˆë‹¤.\n",
    "* ìœ„ì²¨ì $ (i) $ëŠ” $ i^{th} $ í•™ìŠµìë£Œì™€ ê´€ë ¨ëœ ìˆ˜ëŸ‰ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "  - ì˜ˆ : $ x^{(i)} $ëŠ” $ i^{th} $ í•™ìŠµ ì˜ˆì…ë‹ˆë‹¤.\n",
    "* ì†Œë¬¸ì $ i $ëŠ” ë²¡í„°ì˜ $ i^{th} $ í•­ëª©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "  - ì˜ˆ : $ a^{[l]}_i $ëŠ” $ l^{th} $ ë ˆì´ì–´ í™œì„±í™”ì˜ $ i^{th} $ í•­ëª©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "ì‹œì‘í•©ì‹œë‹¤!\n",
    "\n",
    "\n",
    "### 1. íŒ¨í‚¤ì§€\n",
    "\n",
    "ë¨¼ì € ì´ ê³¼ì œ ì¤‘ì— í•„ìš”í•œ ëª¨ë“  íŒ¨í‚¤ì§€ë¥¼ ê°€ì ¸ ì˜¤ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "* numpyëŠ” Pythonì„ ì‚¬ìš©í•œ ê³¼í•™ ì»´í“¨íŒ…ì„ìœ„í•œ ê¸°ë³¸ íŒ¨í‚¤ì§€ì…ë‹ˆë‹¤.\n",
    "* matplotlibëŠ” Pythonì—ì„œ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "* dnn_utilsëŠ”ì´ ë…¸íŠ¸ë¶ì— í•„ìš”í•œ ëª‡ ê°€ì§€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "* testCasesëŠ” ê¸°ëŠ¥ì˜ ì •í™•ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ëª‡ ê°€ì§€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "* np.random.seed(1)ëŠ” ëª¨ë“  ì„ì˜ì˜ í•¨ìˆ˜ í˜¸ì¶œì„ ì¼ê´€ë˜ê²Œ ìœ ì§€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ê·€í•˜ì˜ ì‘ì—…ì„ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. seedë¥¼ ë°”ê¾¸ì§€ ë§ˆì‹­ì‹œì˜¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v4a import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ê³¼ì œ ê°œìš”\n",
    "\n",
    "ì‹ ê²½ë§ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ \"ë„ìš°ë¯¸ í•¨ìˆ˜\"ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì´ ë„ìš°ë¯¸ í•¨ìˆ˜ëŠ” ë‹¤ìŒ ê³¼ì œì—ì„œ 2-ì¸µ ì‹ ê²½ë§ê³¼ L-ì¸µ ì‹ ê²½ë§ì„ êµ¬ì¶•í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. êµ¬í˜„í•  ê° ì‘ì€ ë„ìš°ë¯¸ í•¨ìˆ˜ì—ëŠ” í•„ìš”í•œ ë‹¨ê³„ë¥¼ ì•ˆë‚´í•˜ëŠ” ìì„¸í•œ ì§€ì¹¨ì´ ìˆìŠµë‹ˆë‹¤. ì´ ê³¼ì œì˜ ê°œìš”ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "* 2-ì¸µ ë„¤íŠ¸ì›Œí¬ ë° $L$-ì¸µ ì‹ ê²½ë§ì— ëŒ€í•œ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™” í•©ë‹ˆë‹¤.\n",
    "* ìˆœë°©í–¥ ì „íŒŒ ëª¨ë“ˆì„ êµ¬í˜„í•©ë‹ˆë‹¤ (ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë³´ë¼ìƒ‰ìœ¼ë¡œ í‘œì‹œë¨).\n",
    "  - ë ˆì´ì–´ì˜ ìˆœë°©í–¥ ì „íŒŒ ë‹¨ê³„ì˜ LINEAR ë¶€ë¶„ì„ ì™„ë£Œí•©ë‹ˆë‹¤ (ê²°ê³¼ì ìœ¼ë¡œ $Z^{[l]}$).\n",
    "  - ACTIVATION í•¨ìˆ˜ (relu / sigmoid)ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "  - ì´ì „ ë‘ ë‹¨ê³„ë¥¼ ìƒˆë¡œìš´ [LINEAR-> ACTIVATION] forward í•¨ìˆ˜ë¡œ ê²°í•©í•©ë‹ˆë‹¤.\n",
    "  - [LINEAR-> RELU] forward í•¨ìˆ˜ë¥¼ L-1 ë²ˆ(ë ˆì´ì–´ 1ë¶€í„° L-1ê¹Œì§€) ìŠ¤íƒí•˜ê³  ëì— [LINEAR-> SIGMOID]ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤ (ìµœì¢… ë ˆì´ì–´ $ L $). ì´ê²ƒì€ ìƒˆë¡œìš´ L_model_forward í•¨ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "* ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "* ì—­ë°©í–¥ ì „íŒŒ ëª¨ë“ˆì„ êµ¬í˜„í•©ë‹ˆë‹¤ (ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œë¨).\n",
    "  - ë ˆì´ì–´ì˜ ì—­ë°©í–¥ ì „íŒŒ ë‹¨ê³„ì˜ LINEAR ë¶€ë¶„ì„ ì™„ë£Œí•©ë‹ˆë‹¤.\n",
    "  - ACTIVATION í•¨ìˆ˜ (relu_backward / sigmoid_backward)ì˜ ê¸°ìš¸ê¸°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "  - ì´ì „ ë‘ ë‹¨ê³„ë¥¼ ìƒˆë¡œìš´ [LINEAR-> ACTIVATION] ì—­ë°©í–¥ í•¨ìˆ˜ë¡œ ê²°í•©í•©ë‹ˆë‹¤.\n",
    "  - [LINEAR-> RELU]ë¥¼ L-1 ë²ˆ ë’¤ë¡œ ìŠ¤íƒí•˜ê³  ìƒˆ L_model_backward í•¨ìˆ˜ì—ì„œ [LINEAR-> SIGMOID]ë¥¼ ë’¤ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "* ë§ˆì§€ë§‰ìœ¼ë¡œ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ì‹­ì‹œì˜¤.\n",
    "\n",
    "<img src=\"./images/final%20outline.png\" width=\"600\">\n",
    "\n",
    "**ì£¼ì˜** ëª¨ë“  ìˆœë°©í–¥ í•¨ìˆ˜ì—ëŠ” í•´ë‹¹í•˜ëŠ” ì—­ë°©í–¥ í•¨ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ forward ëª¨ë“ˆì˜ ëª¨ë“  ë‹¨ê³„ì—ì„œ ìºì‹œì— ì¼ë¶€ ê°’ì„ ì €ì¥í•˜ê²Œë©ë‹ˆë‹¤. ìºì‹œ ëœ ê°’ì€ ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚°ì— ìœ ìš©í•©ë‹ˆë‹¤. ì—­ì „íŒŒ ëª¨ë“ˆì—ì„œ ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ê³¼ì œëŠ” ì´ëŸ¬í•œ ê° ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì •í™•í•˜ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "### 3. ì´ˆê¸°í™”\n",
    "ëª¨ë¸ì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ë‘ ê°œì˜ ë„ìš°ë¯¸ í•¨ìˆ˜ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ í•¨ìˆ˜ëŠ” 2-ì¸µ ëª¨ë¸ì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ëŠ” ì´ ì´ˆê¸°í™” í”„ë¡œì„¸ìŠ¤ë¥¼ $ L $ ë ˆì´ì–´ë¡œ ì¼ë°˜í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### 3.1. 2 ì¸µ ì‹ ê²½ë§\n",
    "**(1) ì—°ìŠµë¬¸ì œ** : 2-ì¸µ ì‹ ê²½ë§ì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ë§Œë“¤ê³  ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë°©ë²•**:\n",
    "\n",
    "* ëª¨ë¸ì˜ êµ¬ì¡°ëŠ” LINEAR-> RELU-> LINEAR-> SIGMOIDì…ë‹ˆë‹¤.\n",
    "* ê°€ì¤‘ì¹˜ í–‰ë ¬ì— ì„ì˜ ì´ˆê¸°í™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. np.random.randn(shape) * 0.01ì„ ì˜¬ë°”ë¥¸ ëª¨ì–‘ìœ¼ë¡œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.\n",
    "* í¸í–¥ì— ëŒ€í•´ 0 ì´ˆê¸°í™”ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤. np.zeros(shape)ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
    "    W1 =  \n",
    "    b1 =  \n",
    "    W2 =  \n",
    "    b2 =  \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td> [[ 0.01624345 -0.00611756 -0.00528172]\n",
    " [-0.01072969  0.00865408 -0.02301539]] </td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> **b1**</td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[ 0.01744812 -0.00761207]]</td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> **b2** </td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. L ì¸µ ì‹ ê²½ë§\n",
    "\n",
    "ë” ê¹Šì€ L ê³„ì¸µ ì‹ ê²½ë§ì˜ ì´ˆê¸°í™”ëŠ” ë” ë§ì€ ê°€ì¤‘ì¹˜ í–‰ë ¬ê³¼ í¸í–¥ ë²¡í„°ê°€ ìˆê¸° ë•Œë¬¸ì— ë” ë³µì¡í•©ë‹ˆë‹¤. initialize_parameters_deepì„ ì™„ë£Œ í•  ë•Œ ê° ë ˆì´ì–´ ê°„ì— ì°¨ì›ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. $n^{[l]}$ëŠ” $l$ ë ˆì´ì–´ì˜ ìœ ë‹› ìˆ˜ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì˜ˆë¥¼ ë“¤ì–´ ì…ë ¥ $X$ì˜ í¬ê¸°ê°€ $ (12288, 209)$ ($ m = 209 $ ì˜ˆì œ í¬í•¨)ì´ë©´ :\n",
    "\n",
    "<table style=\"width:100%\" border=\"1px\" >\n",
    "<tr>\n",
    "<td width=\"12%\">  </td> \n",
    "<td width=\"18%\"> Shape of W </td> \n",
    "<td width=\"14%\"> Shape of b  </td> \n",
    "<td width=\"36%\"> Activation </td>\n",
    "<td width=\"20%\"> Shape of Activation </td> \n",
    "<tr>\n",
    "<tr>\n",
    "<td> Layer 1 </td> \n",
    "<td> $(n^{[1]},12288)$ </td> \n",
    "<td> $(n^{[1]},1)$ </td> \n",
    "<td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "<td> $(n^{[1]},209)$ </td> \n",
    "<tr>\n",
    "<tr>\n",
    "<td> Layer 2 </td> \n",
    "<td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "<td> $(n^{[2]},1)$ </td> \n",
    "<td> $Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "<td> $(n^{[2]}, 209)$ </td>\n",
    "<tr>\n",
    "<tr>\n",
    "<td> $\\vdots$ </td> \n",
    "<td> $\\vdots$  </td> \n",
    "<td> $\\vdots$  </td>\n",
    "<td> $\\vdots$</td> \n",
    "<td> $\\vdots$  </td> \n",
    "<tr> \n",
    "<tr>\n",
    "<td> Layer L-1 </td> \n",
    "<td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "<td> $(n^{[L-1]}, 1)$  </td> \n",
    "<td> $Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "<td> $(n^{[L-1]}, 209)$ </td> \n",
    "<tr>\n",
    "<tr>\n",
    "<td> Layer L </td> \n",
    "<td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "<td> $(n^{[L]}, 1)$ </td>\n",
    "<td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "<td> $(n^{[L]}, 209)$  </td> \n",
    "<tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "íŒŒì´ì¬ì—ì„œ $ WX + b $ë¥¼ ê³„ì‚°í•  ë•Œ ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ ìˆ˜í–‰í•œë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì‹­ì‹œì˜¤. ì˜ˆë¥¼ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "ê·¸ëŸ¬ë©´, $WX + b$ ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë©ë‹ˆë‹¤:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$\n",
    "\n",
    "**(2) ì—°ìŠµë¬¸ì œ** : L-ì¸µ ì‹ ê²½ë§ì— ëŒ€í•œ ì´ˆê¸°í™”ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë°©ë²•**:\n",
    "\n",
    "* ëª¨ë¸ì˜ êµ¬ì¡°ëŠ” [LINEAR-> RELU] $ \\times $ (L-1)-> LINEAR-> SIGMOIDì…ë‹ˆë‹¤. ì¦‰, ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” $ L-1 $ ë ˆì´ì–´ì™€ ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜ê°€ ìˆëŠ” ì¶œë ¥ ë ˆì´ì–´ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "* ê°€ì¤‘ì¹˜ í–‰ë ¬ì— ì„ì˜ ì´ˆê¸°í™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. np.random.randn(shape) * 0.01ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "* í¸í–¥ì— ëŒ€í•´ 0 ì´ˆê¸°í™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. np.zeros(shape)ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.\n",
    "* ë‹¤ë¥¸ ë ˆì´ì–´ì˜ ë‹¨ìœ„ ìˆ˜ì¸ $ n^{[l]} $ë¥¼ ë³€ìˆ˜ layer_dimsì— ì €ì¥í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì§€ë‚œì£¼ \"í‰ë©´ ë°ì´í„° ë¶„ë¥˜ ëª¨ë¸\"ì— ëŒ€í•œ layer_dimsëŠ” [2,4,1]ì´ ë©ë‹ˆë‹¤. ì…ë ¥ì´ 2 ê°œ ìˆì—ˆëŠ”ë°, í•˜ë‚˜ì—ëŠ” 4 ê°œì˜ ì€ë‹‰ ìœ ë‹›ì´ ìˆê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” 1 ê°œì˜ ì¶œë ¥ ìœ ë‹›ì´ ìˆëŠ” ì¶œë ¥ ë ˆì´ì–´ì˜€ìŠµë‹ˆë‹¤. ì´ê²ƒì€ W1ì˜ ëª¨ì–‘ì´ (4,2), b1ì´ (4,1), W2ê°€ (1,4)ì´ê³  b2ê°€ (1,1)ì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ì œ ì´ê²ƒì„ $ L $ ë ˆì´ì–´ë¡œ ì¼ë°˜í™”í•©ë‹ˆë‹¤!\n",
    "* ë‹¤ìŒì€ $ L = 1 $ (1 ì¸µ ì‹ ê²½ë§)ì— ëŒ€í•œ êµ¬í˜„ì…ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ê²½ìš° (L ì¸µ ì‹ ê²½ë§)ë¥¼ êµ¬í˜„í•˜ë„ë¡ ì˜ê°ì„ ì¤„ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "if L == 1:\n",
    "      parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "      parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        parameters['W' + str(l)] =  \n",
    "        parameters['b' + str(l)] =  \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td width=\"20%\"> W1 </td>\n",
    "    <td width=\"60%\">[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>b1 </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>W2 </td>\n",
    "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>b2 </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-ìˆœë°©í–¥ ì „íŒŒ ëª¨ë“ˆ\n",
    "### 4.1-ì„ í˜• ìˆœë°©í–¥\n",
    "ì´ì œ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™” í–ˆìœ¼ë¯€ë¡œ ìˆœë°©í–¥ ì „íŒŒ ëª¨ë“ˆì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë‚˜ì¤‘ì— ëª¨ë¸ì„ êµ¬í˜„í•  ë•Œ ì‚¬ìš©í•  ëª‡ ê°€ì§€ ê¸°ë³¸ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œë¡œ ì„¸ ê°€ì§€ í•¨ìˆ˜ë¥¼ ì™„ë£Œí•©ë‹ˆë‹¤.\n",
    "\n",
    "* LINEAR\n",
    "* LINEAR -> ACTIVATION ì—¬ê¸°ì„œ ACTIVATIONì€ ReLU ë˜ëŠ” Sigmoidì…ë‹ˆë‹¤.\n",
    "* [LINEAR-> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID (ì „ì²´ ëª¨ë¸)\n",
    "\n",
    "ì„ í˜• ìˆœë°©í–¥ ëª¨ë“ˆ(ëª¨ë“  ì˜ˆì— ëŒ€í•˜ì—¬ ë²¡í„°í™” ëœ)ì€ ë‹¤ìŒ ë°©ì •ì‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "ì—¬ê¸°ì—ì„œ  $A^{[0]} = X$.\n",
    "\n",
    "**(3) ì—°ìŠµë¬¸ì œ** : ìˆœë°©í–¥ ì „íŒŒì˜ ì„ í˜• ë¶€ë¶„ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ì˜** : ì´ ìœ ë‹›ì˜ ìˆ˜í•™ì  í‘œí˜„ì€ $ Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]} $ì…ë‹ˆë‹¤. np.dot( ) ì´ ìœ ìš© í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì°¨ì›ì´ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ W.shape ì„ ì¶œë ¥í•˜ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "    Z =  \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "<table style=\"width:40%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td width=\"15%\"> Z = </td>\n",
    "    <td> [[ 3.26295337 -1.23429987]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2- ì„ í˜•-í™œì„±í™” ìˆœë°©í–¥\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ë‘ ê°€ì§€ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "* **ì‹œê·¸ëª¨ì´ë“œ** : $ \\sigma(Z) = \\sigma(WA + b) = \\frac {1}{1 + e^{-(WA + b)}} $. ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” í™œì„±í™” ê°’ \"a\"ì™€ \"Z\"ë¥¼ í¬í•¨í•˜ëŠ” \"cache\"ë¼ëŠ” ë‘ í•­ëª©ì„ ë°˜í™˜í•©ë‹ˆë‹¤ (í•´ë‹¹í•˜ëŠ” ì—­ë°©í–¥ í•¨ìˆ˜ì— ì œê³µ í•  ê²ƒì…ë‹ˆë‹¤). ê·¸ê²ƒì„ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒì„ í˜¸ì¶œí•˜ì‹­ì‹œì˜¤.\n",
    "\n",
    "     A, activation_cache = sigmoid(Z)\n",
    "     \n",
    "\n",
    "* **ReLU** : ReLUì˜ ìˆ˜í•™ ê³µì‹ì€ $ A = RELU(Z) = max(0, Z) $ì…ë‹ˆë‹¤. relu í•¨ìˆ˜ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” í™œì„±í™” ê°’ \"A\"ì™€ \"Z\"ë¥¼ í¬í•¨í•˜ëŠ” \"cache\"ë¼ëŠ” ë‘ í•­ëª©ì„ ë°˜í™˜í•©ë‹ˆë‹¤ (í•´ë‹¹í•˜ëŠ” ì—­ë°©í–¥ í•¨ìˆ˜ì— ì œê³µí•  ê²ƒì…ë‹ˆë‹¤). ê·¸ê²ƒì„ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒìœ¼ë¡œ í˜¸ì¶œí•˜ì‹­ì‹œì˜¤.\n",
    "\n",
    "     A, activation_cache = relu(Z)\n",
    "\n",
    "\n",
    "í¸ì˜ë¥¼ ìœ„í•´ ë‘ ê¸°ëŠ¥ (Linear ë° Activation)ì„ í•˜ë‚˜ì˜ ê¸°ëŠ¥ (LINEAR-> ACTIVATION)ìœ¼ë¡œ ê·¸ë£¹í™” í•  ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ LINEAR ì•ìœ¼ë¡œ ë‹¨ê³„ë¥¼ ìˆ˜í–‰ í•œ ë‹¤ìŒ ACTIVATION ì•ìœ¼ë¡œ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "**(3) ì—°ìŠµë¬¸ì œ**  : LINEAR-> ACTIVATION ë ˆì´ì–´ì˜ ìˆœë°©í–¥ ì „íŒŒë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ìˆ˜í•™ì  ê´€ê³„ : $ A ^ {[l]} = g (Z ^ {[l]}) = g (W ^ {[l]} A ^ {[l-1]} + b ^ {[l]} ) $ ì—¬ê¸°ì„œ í™œì„±í™” \"g\"ëŠ” sigmoid() ë˜ëŠ” relu() ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. linear_forward() ë° ì˜¬ë°”ë¥¸ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        Z, linear_cache =  \n",
    "        A, activation_cache =  \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        Z, linear_cache =  \n",
    "        A, activation_cache = \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table width=\"50%\">\n",
    "  <tr>\n",
    "    <td width=\"25%\">  With sigmoid: A  </td>\n",
    "    <td > [[ 0.96890023  0.11013289]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> With ReLU: A   </td>\n",
    "    <td > [[ 3.43896131  0.        ]]</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "ì°¸ê³  : ë”¥ ëŸ¬ë‹ì—ì„œ \"[LINEAR-> ACTIVATION]\"ê³„ì‚°ì€ ì‹ ê²½ë§ì—ì„œ ë‘ ê³„ì¸µì´ ì•„ë‹Œ ë‹¨ì¼ ê³„ì¸µìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) L ì¸µ ëª¨ë¸\n",
    "$L$-ì¸µ Neural Netì„ êµ¬í˜„í•  ë•Œ ë” ë§ì€ í¸ì˜ë¥¼ ìœ„í•´ ì´ì „ í•¨ìˆ˜ (RELUë¥¼ ì‚¬ìš©í•œ linear_activation_forward)ë¥¼ $ L-1 $ ë²ˆ ë³µì œ í•œ ë‹¤ìŒ SIGMOIDë¥¼ ì‚¬ìš©í•˜ì—¬ linear_activation_forwardë¥¼ í•œ ë²ˆ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"./images/model_architecture_kiank.png\">\n",
    "\n",
    "\n",
    "**(4) ì—°ìŠµë¬¸ì œ** : ìœ„ ëª¨ë¸ì˜ ìˆœë°©í–¥ ì „íŒŒë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì§€ì‹œ** : ì•„ë˜ ì½”ë“œì—ì„œ AL ë³€ìˆ˜ëŠ” $ A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]}) $. (ë•Œë•Œë¡œ Yhatì´ë¼ê³ ë„í•©ë‹ˆë‹¤. ì¦‰, $ \\hat{Y} $ì…ë‹ˆë‹¤.)\n",
    "\n",
    "**íŒ** :\n",
    "\n",
    "* ì´ì „ì— ì‘ì„±í•œ í•¨ìˆ˜ ì‚¬ìš©\n",
    "* for ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ [LINEAR-> RELU] (L-1) ë²ˆ ë³µì œ\n",
    "* \"caches\"ëª©ë¡ì—ì„œ ìºì‹œë¥¼ ì¶”ì í•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì‹­ì‹œì˜¤. ëª©ë¡ì— ìƒˆ ê°’ cë¥¼ ì¶”ê°€í•˜ë ¤ë©´ list.append(c)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        A, cache =  \n",
    "        caches.append(    )\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "    AL, cache =   \n",
    "    caches.append(    )\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <td width=\"30%\"> AL </td>\n",
    "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td > Length of caches list  </td>\n",
    "    <td > 3 </td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "í›Œë¥­í•©ë‹ˆë‹¤! ì´ì œ ì…ë ¥ Xë¥¼ ê°€ì ¸ì™€ ì˜ˆì¸¡ì„ í¬í•¨í•˜ëŠ” í–‰ ë²¡í„° $ A^{[L]} $ë¥¼ ì¶œë ¥í•˜ëŠ” ì „ì²´ ìˆœë°©í–¥ ì „íŒŒê°€ ìˆìŠµë‹ˆë‹¤. ë˜í•œ \"caches\"ì— ëª¨ë“  ì¤‘ê°„ ê°’ì„ ê¸°ë¡í•©ë‹ˆë‹¤. $ A^{[L]} $ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ë¹„ìš©ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-ë¹„ìš© í•¨ìˆ˜\n",
    "ì´ì œ ìˆœë°©í–¥ ë° ì—­ë°©í–¥ ì „íŒŒë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ì‹¤ì œë¡œ í•™ìŠµ ì¤‘ì¸ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ë¹„ìš©ì„ ê³„ì‚°í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "\n",
    "**(5) ì—°ìŠµë¬¸ì œ**: ë‹¤ìŒ ê³µì‹ì„ ì‚¬ìš©í•˜ì—¬ êµì°¨ ì—”íŠ¸ë¡œí”¼ ë¹„ìš© $ J $ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. \n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (â‰ˆ 1 lines of code)\n",
    "    cost =  \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <td>**cost** </td>\n",
    "    <td> 0.2797765635793422</td> \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### 6-ì—­ë°©í–¥ ì „íŒŒ ëª¨ë“ˆ\n",
    "ìˆœë°©í–¥ ì „íŒŒì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì—­ ì „íŒŒë¥¼ìœ„í•œ ë„ìš°ë¯¸ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ì—­ì „íŒŒëŠ” ë§¤ê°œ ë³€ìˆ˜ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì¡°ì–¸:\n",
    "<img src=\"./images/backprop_kiank.png\">\n",
    "\n",
    "ì´ì œ ìˆœë°©í–¥ ì „íŒŒì™€ ìœ ì‚¬í•˜ê²Œ ì„¸ ë‹¨ê³„ë¡œ ì—­ë°©í–¥ ì „íŒŒë¥¼ ë¹Œë“œ í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "* LINEAR ë’¤ë¡œ\n",
    "* LINEAR-> ACTIVATION ë’¤ë¡œ ACTIVATIONì´ ReLU ë˜ëŠ” ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™”ì˜ ë¯¸ë¶„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "* [LINEAR-> RELU] $ \\ times $ (L-1)-> LINEAR-> SIGMOID ë’¤ë¡œ (ì „ì²´ ëª¨ë¸)\n",
    "\n",
    "#### 6.1-ì„ í˜• ì—­ë°©í–¥\n",
    "$ l $ ë ˆì´ì–´ì˜ ê²½ìš° ì„ í˜• ë¶€ë¶„ì€ $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (í™œì„±í™” í›„ ).\n",
    "\n",
    "ë¯¸ë¶„  $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$ë¥¼ ì´ë¯¸ ê³„ì‚°í–ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.$(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ë¥¼ ì–»ê³  ì‹¶ìŠµë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"./images/linearback_kiank.png\" width=\"200\">\n",
    "\n",
    "3 ê°œì˜ ì¶œë ¥  $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ëŠ” ì…ë ¥ $dZ^{[l]}$ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤. í•„ìš”í•œ ê³µì‹ :\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n",
    "\n",
    "**(6) ì—°ìŠµë¬¸ì œ** : ìœ„ì˜ ì„¸ ê°€ì§€ ê³µì‹ì„ ì‚¬ìš©í•˜ì—¬ linear_backward()ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    dW =  \n",
    "    db =  \n",
    "    dA_prev =  \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "    \n",
    "```\n",
    "dA_prev = \n",
    " [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
    " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
    " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
    " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
    " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
    "dW = \n",
    " [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
    " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
    " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
    "db = \n",
    " [[-0.14713786]\n",
    " [-0.11313155]\n",
    " [-0.13209101]]\n",
    "```\n",
    "\n",
    "\n",
    " \n",
    "#### 6.2-ì—­ë°©í–¥ ì„ í˜• í™œì„±í™”\n",
    "ë‹¤ìŒìœ¼ë¡œ, linear_backward ë° linear_activation_backward í™œì„±í™”ë¥¼ ìœ„í•œ ë’¤ë¡œ ë‹¨ê³„ë¼ëŠ” ë‘ ë„ìš°ë¯¸ í•¨ìˆ˜ë¥¼ ë³‘í•©í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "linear_activation_backwardë¥¼ êµ¬í˜„í•˜ëŠ” ë° ë„ì›€ì„ ì£¼ê¸° ìœ„í•´ ë‘ ê°œì˜ ì—­ë°©í–¥ í•¨ìˆ˜ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "sigmoid_backward : SIGMOID ë‹¨ìœ„ì— ëŒ€í•œ ì—­ë°©í–¥ ì „íŒŒë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ í˜¸ì¶œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "dZ = sigmoid_backward (dA, í™œì„±í™” _ ìºì‹œ)\n",
    "relu_backward : RELU ìœ ë‹›ì— ëŒ€í•œ ì—­ë°©í–¥ ì „íŒŒë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ í˜¸ì¶œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "dZ = relu_backward (dA, í™œì„±í™” _ ìºì‹œ)\n",
    "$ g (.) $ê°€ í™œì„±í™” í•¨ìˆ˜ì´ë©´ sigmoid_backward ë° relu_backward ê³„ì‚°\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$\n",
    " \n",
    " **(7): ì—°ìŠµë¬¸ì œ** : LINEAR-> ACTIVATION ë ˆì´ì–´ì— ëŒ€í•œ ì—­ì „íŒŒë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        dZ =  \n",
    "        dA_prev, dW, db =  \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        dZ =  \n",
    "        dA_prev, dW, db =  \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output with sigmoid:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td >[[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]] </td> \n",
    "  </tr>  \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td> \n",
    "  </tr>   \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.05729622]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n",
    "**Expected output with relu:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td > [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]] </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.20837892]] </td> \n",
    "  </tr> \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3-L ëª¨ë¸ ì˜ë°©í–¥\n",
    "\n",
    "ì´ì œ ì „ì²´ ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•´ ì—­ë°©í–¥ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. L_model_forward í•¨ìˆ˜ë¥¼ êµ¬í˜„í•  ë•Œ ê° ë°˜ë³µì—ì„œ (X, W, b ë° z)ë¥¼ í¬í•¨í•˜ëŠ” ìºì‹œë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤. ì—­ ì „íŒŒ ëª¨ë“ˆì—ì„œëŠ” ì´ëŸ¬í•œ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ë”°ë¼ì„œ L_model_backward í•¨ìˆ˜ì—ì„œ ë ˆì´ì–´ ğ¿ì—ì„œ ì‹œì‘í•˜ì—¬ ëª¨ë“  ìˆ¨ê²¨ì§„ ë ˆì´ì–´ë¥¼ ë’¤ë¡œ ë°˜ë³µí•©ë‹ˆë‹¤. ê° ë‹¨ê³„ì—ì„œ ë ˆì´ì–´ ğ‘™ì— ëŒ€í•´ ìºì‹œ ëœ ê°’ì„ ì‚¬ìš©í•˜ì—¬ ë ˆì´ì–´ ğ‘™ë¥¼ í†µí•´ ì—­ ì „íŒŒí•©ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ 5ëŠ” ì—­ë°©í–¥ íŒ¨ìŠ¤ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"./images/nm_backward.png\">\n",
    "\n",
    "**ì—­ì „íŒŒ ì´ˆê¸°í™”** :ì´ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ì—­ ì „íŒŒí•˜ë ¤ë©´ ì¶œë ¥ì´  $A^{[L]} = \\sigma(Z^{[L]})$ .ì´ë¼ëŠ” ê²ƒì„ ì•Œê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì½”ë“œëŠ” dAL $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$ ë¥¼ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë ¤ë©´ ë‹¤ìŒ ê³µì‹ì„ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤ (ì‹¬ì¸µ ì§€ì‹ì´ í•„ìš”í•˜ì§€ ì•Šì€ ë¯¸ì ë¶„ì„ ì‚¬ìš©í•˜ì—¬ íŒŒìƒ ë¨).:\n",
    "```\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "```\n",
    "\n",
    "ê·¸ëŸ° ë‹¤ìŒì´ í™œì„±í™” í›„ ê·¸ë¼ë°ì´ì…˜ dALì„ ì‚¬ìš©í•˜ì—¬ ê³„ì† ë’¤ë¡œ ì´ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¼ 5ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ì´ì œ êµ¬í˜„ í•œ LINEAR-> SIGMOID ì—­ë°©í–¥ í•¨ìˆ˜ì— dALì„ ì…ë ¥ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (L_model_forward í•¨ìˆ˜ì— ì €ì¥ëœ ìºì‹œ ëœ ê°’ì„ ì‚¬ìš©í•¨). ê·¸ í›„ LINEAR-> RELU ì—­ë°©í–¥ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë“  ë ˆì´ì–´ë¥¼ ë°˜ë³µí•˜ë ¤ë©´ for ë£¨í”„ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ê° dA, dW ë° dbë¥¼ grads ì‚¬ì „ì— ì €ì¥í•´ì•¼í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë ¤ë©´ ë‹¤ìŒ ê³µì‹ì„ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.:\n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´ $ l = 3 $ì˜ ê²½ìš° $ dW^{[l]} $ë¥¼ grads[\"dW3\"]ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "**(9) ì—°ìŠµë¬¸ì œ** : [LINEAR-> RELU] $ \\ times $ (L-1)-> LINEAR-> SIGMOID ëª¨ë¸ì— ëŒ€í•œ ì—­ ì „íŒŒë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = \n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = \n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = \n",
    "        dA_prev_temp, dW_temp, db_temp = \n",
    "        grads[\"dA\" + str(l)] = \n",
    "        grads[\"dW\" + str(l + 1)] = \n",
    "        grads[\"db\" + str(l + 1)] = \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  <tr>\n",
    "    <td > dW1 </td> \n",
    "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > db1 </td> \n",
    "           <td > [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "  <td > dA1 </td> \n",
    "           <td > [[ 0.12913162 -0.44014127]\n",
    " [-0.14175655  0.48317296]\n",
    " [ 0.01663708 -0.05670698]] </td>\n",
    "  </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4-ì—…ë°ì´íŠ¸ ë§¤ê°œ ë³€ìˆ˜\n",
    "ì´ ì„¹ì…˜ì—ì„œëŠ” ê²½ì‚¬ í•˜ê°• ë²•ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "ì—¬ê¸°ì„œ ğ›¼ì€ í•™ìŠµë¥ ì…ë‹ˆë‹¤. ì—…ë°ì´íŠ¸ ëœ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê³„ì‚° í•œ í›„ ë§¤ê°œ ë³€ìˆ˜ ì‚¬ì „ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "**(10) ì—°ìŠµë¬¸ì œ** : ê²½ì‚¬ í•˜ê°• ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ë ¤ë©´ update_parameters ()ë¥¼ êµ¬í˜„í•˜ì‹­ì‹œì˜¤.\n",
    "\n",
    "ì§€ì¹¨ :  $l = 1, 2, ..., L$.ì— ëŒ€í•´  $W^{[l]}$ ë° $b^{[l]}$ ë§ˆë‹¤ ê²½ì‚¬ í•˜ê°• ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = \n",
    "        parameters[\"b\" + str(l+1)] = \n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "    <tr>\n",
    "    <td > W1 </td> \n",
    "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > b1 </td> \n",
    "           <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > W2 </td> \n",
    "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > b2 </td> \n",
    "           <td > [[-0.84610769]] </td> \n",
    "  </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-ê²°ë¡ \n",
    "ì‹¬ì¸µ ì‹ ê²½ë§ êµ¬ì¶•ì— í•„ìš”í•œ ëª¨ë“  ê¸°ëŠ¥ì„ êµ¬í˜„ í•œ ê²ƒì„ ì¶•í•˜í•©ë‹ˆë‹¤!\n",
    "\n",
    "ê¸´ ì„ë¬´ ì˜€ì§€ë§Œ ì•ìœ¼ë¡œ ë” ë‚˜ì•„ì§ˆ ê²ƒì…ë‹ˆë‹¤. ê³¼ì œì˜ ë‹¤ìŒ ë¶€ë¶„ì´ ë” ì‰½ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒ ê³¼ì œì—ì„œëŠ”ì´ ëª¨ë“  ê²ƒì„ ê²°í•©í•˜ì—¬ ë‘ ê°€ì§€ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "\n",
    "* 2-ì¸µ ì‹ ê²½ë§\n",
    "* L-ì¸µ ì‹ ê²½ë§\n",
    "\n",
    "ì‹¤ì œë¡œ ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê³ ì–‘ì´ ì´ë¯¸ì§€ì™€ ê³ ì–‘ì´ ì´ë¯¸ì§€ê°€ ì•„ë‹Œ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
